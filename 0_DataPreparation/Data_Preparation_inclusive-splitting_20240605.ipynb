{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packes einladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV Datein in dataframes einladen und erste Zeilen aller DF ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"https://raw.githubusercontent.com/FabsenMc/bakery_prediction/main/0_DataPreparation/kiwo.csv\"\n",
    "url2 = \"https://raw.githubusercontent.com/FabsenMc/bakery_prediction/main/0_DataPreparation/umsatzdaten_gekuerzt.csv\"\n",
    "url3 = \"https://raw.githubusercontent.com/FabsenMc/bakery_prediction/main/0_DataPreparation/wetter.csv\"\n",
    "url4 = \"https://raw.githubusercontent.com/FabsenMc/bakery_prediction/main/0_DataPreparation/Wettercodes.csv\"\n",
    "url5 = \"https://raw.githubusercontent.com/FabsenMc/bakery_prediction/main/0_DataPreparation/fuf_v2.csv\"\n",
    "url6 = \"https://raw.githubusercontent.com/FabsenMc/bakery_prediction/main/0_DataPreparation/thw-kiel-spieltage.csv\"\n",
    "url7 = \"https://raw.githubusercontent.com/FabsenMc/bakery_prediction/main/0_DataPreparation/kieler_umschlag.csv\"\n",
    "url8 = \"https://raw.githubusercontent.com/FabsenMc/bakery_prediction/main/0_DataPreparation/weihnachtsmarkt.csv\"\n",
    "\n",
    "# Überführen der Daten in DataFrames\n",
    "daten = pd.read_csv(url1) # Daten der Kiwo\n",
    "umswar = pd.read_csv(url2) # Umsätze der Warengruppen\n",
    "wetter = pd.read_csv(url3) # Wetterdaten der Kiwos\n",
    "wetterc = pd.read_csv(url4) # Wettercodes\n",
    "ferien = pd.read_csv(url5) # Feriendaten\n",
    "thw = pd.read_csv(url6) # THW Kiel Spieltage\n",
    "kium = pd.read_csv(url7) # Kieler Umschlag Tage\n",
    "weima = pd.read_csv(url8) # Weihnachtsmarkt Tage\n",
    "\n",
    "# Anzeige der ersten Zeilen der DataFrames\n",
    "print(daten.head()) # Ausgabe der ersten 5 Zeilen\n",
    "print(umswar.head()) # Ausgabe der ersten 5 Zeilen \n",
    "print(wetter.head()) # Ausgabe der ersten 5 Zeilen\n",
    "print(wetterc.head()) # Ausgabe der ersten 5 Zeilen\n",
    "print(ferien.head()) # Ausgabe der ersten 5 Zeilen\n",
    "print(thw.head()) # Ausgabe der ersten 5 Zeilen\n",
    "print(kium.head()) # Ausgabe der ersten 5 Zeilen\n",
    "print(weima.head()) # Ausgabe der ersten 5 Zeilen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurieren der Datenformate und mergen der DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date format from DD.MM.YYYY to MM/DD/YYYY\n",
    "ferien['Datum'] = pd.to_datetime(ferien['Datum'], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Die 3 DataFrames zusammenführen (mergen) in einen neuen gemeinsamen DataFrame mit der Methode \"outer\"\n",
    "dataf = daten.merge(umswar, on=\"Datum\", how = \"outer\") \\\n",
    "             .merge(wetter, on=\"Datum\", how = \"outer\") \\\n",
    "             .merge(wetterc, on=\"Wettercode\", how = \"outer\") \\\n",
    "             .merge(ferien, on=\"Datum\", how = \"outer\") \\\n",
    "             .merge(thw, on=\"Datum\", how = \"outer\") \\\n",
    "             .merge(kium, on=\"Datum\", how = \"outer\") \\\n",
    "             .merge(weima, on=\"Datum\", how = \"outer\")\n",
    "\n",
    "# Ausgabe der ersten 5 Zeilen des neuen DataFrames\n",
    "print(dataf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinzufügen einer zusätzlichen Spalte Regen ja/nein\n",
    "dataf[\"Regen\"] = dataf[\"Beschreibung\"].str.contains(\"Regen\", case=False, na=False)\n",
    "dataf[\"Regen\"] = dataf[\"Regen\"].map({True: 1, False: 0})\n",
    "\n",
    "# Hinzufügen einer zusätzlichen Spalte mit den Wochentagen\n",
    "dataf[\"Datum\"] = pd.to_datetime(dataf[\"Datum\"])\n",
    "dataf[\"Wochentag_MDMDFSS\"] = dataf[\"Datum\"].dt.weekday\n",
    "#dataf[\"Wochentag\"] = dataf[\"Wochentag\"].map({0: \"Montag\", 1: \"Dienstag\", 2: \"Mittwoch\", 3: \"Donnerstag\", 4: \"Freitag\", 5: \"Samstag\", 6: \"Sonntag\"}) #-> der ML Algorythmus kann ja nur mit Zahlen umgehen\n",
    "\n",
    "# Hinzufügen einer zusätzlichen Spalte mit den Wochenenden\n",
    "dataf[\"Wochenende\"] = dataf[\"Wochentag_MDMDFSS\"].map({0 : 0, 1 : 0, 2 : 0, 3 : 0, 4 : 0, 5 : 1, 6 : 1})\n",
    "\n",
    "# Hinzufügen einer zusätzlichen Spalte mit den Jahreszeiten Frühling Sommer Herbst und Winter (FSHW) in Abhängigkeit des Datums\n",
    "dataf[\"Jahreszeit_FSHW\"] = dataf[\"Datum\"].dt.month\n",
    "dataf[\"Jahreszeit_FSHW\"] = dataf[\"Datum\"].dt.month.map({1: 4, 2: 4, 3: 1, 4: 1, 5: 1, 6: 2, 7: 2, 8: 2, 9: 3, 10: 3, 11: 3, 12: 4})\n",
    "\n",
    "# Hinzufügen einer zusätzlichen Spalte Temperatur Kategorie # min -10, max: 32\n",
    "if 'Temperatur_Kategorie' in dataf.columns:\n",
    "    dataf.drop(columns=['Temperatur_Kategorie'], inplace=True)\n",
    "# Define the bin edges\n",
    "bins = [-10, 10, 20, 35]\n",
    "# Define the bin labels\n",
    "labels = ['Niedrig', 'Mittel', 'Hoch']\n",
    "\n",
    "# Use pd.cut() to create the new column \"Temperatur_Kategorie\"\n",
    "dataf['Temperatur_Kategorie'] = pd.cut(dataf['Temperatur'], bins=bins, labels=labels)\n",
    "\n",
    "# Ausgabe der ersten 5 Zeilen des gemergten DataFrames\n",
    "print(dataf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichern des DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf.to_csv('dataf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einladen des DFs, Splitting des DF in Trainings- und Validierungsdaten und Abspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des Dataframes -> Gibt es hier ein bessere Version?\n",
    "dataf = pd.read_csv('https://raw.githubusercontent.com/FabsenMc/bakery_prediction/main/0_DataPreparation/dataf.csv')\n",
    "print(dataf.head())\n",
    "\n",
    "#DF nach Datum sortieren\n",
    "dataf = dataf.sort_values(by='Datum')\n",
    "\n",
    "# Definieren der Datumsgrenzen\n",
    "train_start_date = '2013-07-01'\n",
    "train_end_date = '2017-07-31'\n",
    "validation_end_date = '2018-07-31'\n",
    "\n",
    "# Splitten der Daten basierend auf den Datumsgrenzen\n",
    "train_data = dataf[(dataf['Datum']>= train_start_date) & (dataf['Datum'] <= train_end_date)]\n",
    "validation_data = dataf[(dataf['Datum'] > train_end_date) & (dataf['Datum'] <= validation_end_date)]\n",
    "\n",
    "# Überprüfen der Dimensionen der Datensätze\n",
    "print(\"Training dataset dimensions:\", train_data.shape)\n",
    "print(\"Validation dataset dimensions:\", validation_data.shape)\n",
    "\n",
    "#Abspeichern der Datensätze als CSV\n",
    "#dataf.to_csv('dataf.csv', index=False)\n",
    "train_data.to_csv ('train_data.csv', index=False)\n",
    "validation_data.to_csv('validation_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
