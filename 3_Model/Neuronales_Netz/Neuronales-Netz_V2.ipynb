{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation für das Neuronale Netz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importieren der Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importieren des Datensatzes sowie Aufteilung der Umsätze in eine Spalte pro Warengruppe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datum                   datetime64[ns]\n",
      "KielerWoche                    float64\n",
      "Warengruppe                    float64\n",
      "Umsatz                         float64\n",
      "Bewoelkung                     float64\n",
      "Temperatur                     float64\n",
      "Windgeschwindigkeit            float64\n",
      "Wettercode                     float64\n",
      "Beschreibung                    object\n",
      "FerienSH                       float64\n",
      "Feiertag                       float64\n",
      "Uhrzeit                         object\n",
      "Heim_Auswärts                   object\n",
      "Umschlag                       float64\n",
      "Weihnachtsmarkt                float64\n",
      "Regen                            int64\n",
      "Wochentag_MDMDFSS                int64\n",
      "Wochenende                       int64\n",
      "Jahreszeit_FSHW                  int64\n",
      "Temperatur_Kategorie            object\n",
      "dtype: object\n",
      "       Datum  Warengruppe_1.0  Warengruppe_2.0  Warengruppe_3.0  \\\n",
      "0 2013-07-01       148.828353       535.856285       201.198426   \n",
      "1 2013-07-02       159.793757       546.780787       265.261254   \n",
      "2 2013-07-03       111.885594       427.343259       210.260241   \n",
      "3 2013-07-04       168.864941       454.859641       190.686641   \n",
      "4 2013-07-05       171.280754       492.818804       181.644870   \n",
      "\n",
      "   Warengruppe_4.0  Warengruppe_5.0  Warengruppe_6.0  \n",
      "0        65.890169       317.475875              0.0  \n",
      "1        74.543917       383.628682              0.0  \n",
      "2        69.262728       305.523072              0.0  \n",
      "3        61.490175       308.408168              0.0  \n",
      "4        86.759861       355.518770              0.0  \n",
      "Datum              datetime64[ns]\n",
      "Warengruppe_1.0           float64\n",
      "Warengruppe_2.0           float64\n",
      "Warengruppe_3.0           float64\n",
      "Warengruppe_4.0           float64\n",
      "Warengruppe_5.0           float64\n",
      "Warengruppe_6.0           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Laden des Datensatzes\n",
    "dataf = pd.read_csv(\"https://raw.githubusercontent.com/FabsenMc/bakery_prediction/main/0_DataPreparation/dataf.csv\")\n",
    "\n",
    "# Convert the date column to datetime (sicherheitshalber)\n",
    "dataf['Datum'] = pd.to_datetime(dataf['Datum'])\n",
    "\n",
    "# prüfen welche Spalten in Categories umgewandelt werden müssen\n",
    "print(dataf.dtypes)\n",
    "\n",
    "# Pivot the data to have one row per date and one column per category\n",
    "pivot_dataf = dataf.pivot_table(index='Datum', columns='Warengruppe', values='Umsatz').reset_index()\n",
    "\n",
    "#New created columns get the prefix 'Warengruppe_'\n",
    "pivot_dataf.columns = ['Datum'] + ['Warengruppe_' + str(col) for col in pivot_dataf.columns if col != 'Datum']\n",
    "\n",
    "# Ensure no missing values by filling them with zeros (hier vmtl wichtig für Warengruppe 6)\n",
    "pivot_dataf.fillna(0, inplace=True)\n",
    "\n",
    "# Change the type of all columns with the prefix 'Warengruppe_' to float\n",
    "for col in pivot_dataf.columns:\n",
    "    if 'Warengruppe_' in col:\n",
    "        pivot_dataf[col] = pivot_dataf[col].astype(float)\n",
    "\n",
    "# Anzeigen lassen ob die Pivotierung funktioniert hat und wie die Spalten heißen\n",
    "print(pivot_dataf.head())\n",
    "print(pivot_dataf.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kategorielle Variablen definieren und mit dem restlichen Datensatz vereinigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values:\n",
      " Temperatur_Kategorie                         [Niedrig, nan, Mittel, Hoch]\n",
      "Bewoelkung              [8.0, 7.0, 4.0, 6.0, 3.0, 2.0, 1.0, 5.0, 0.0, ...\n",
      "dtype: object\n",
      "Prepared data shape:\n",
      " (1819, 17)\n",
      "Prepared data first rows:\n",
      "        Datum  Warengruppe_1.0  Warengruppe_2.0  Warengruppe_3.0  \\\n",
      "0 2013-07-01       148.828353       535.856285       201.198426   \n",
      "1 2013-07-02       159.793757       546.780787       265.261254   \n",
      "2 2013-07-03       111.885594       427.343259       210.260241   \n",
      "3 2013-07-04       168.864941       454.859641       190.686641   \n",
      "4 2013-07-05       171.280754       492.818804       181.644870   \n",
      "\n",
      "   Warengruppe_4.0  Warengruppe_5.0  Warengruppe_6.0  \\\n",
      "0        65.890169       317.475875              0.0   \n",
      "1        74.543917       383.628682              0.0   \n",
      "2        69.262728       305.523072              0.0   \n",
      "3        61.490175       308.408168              0.0   \n",
      "4        86.759861       355.518770              0.0   \n",
      "\n",
      "   Temperatur_Kategorie_Mittel  Temperatur_Kategorie_Niedrig  Bewoelkung_1.0  \\\n",
      "0                            0                             1               0   \n",
      "1                            0                             1               0   \n",
      "2                            0                             1               0   \n",
      "3                            0                             1               0   \n",
      "4                            0                             1               0   \n",
      "\n",
      "   Bewoelkung_2.0  Bewoelkung_3.0  Bewoelkung_4.0  Bewoelkung_5.0  \\\n",
      "0               0               0               0               0   \n",
      "1               0               0               0               0   \n",
      "2               0               0               0               0   \n",
      "3               0               0               1               0   \n",
      "4               0               0               0               0   \n",
      "\n",
      "   Bewoelkung_6.0  Bewoelkung_7.0  Bewoelkung_8.0  \n",
      "0               0               0               1  \n",
      "1               0               1               0  \n",
      "2               0               0               1  \n",
      "3               0               0               0  \n",
      "4               1               0               0  \n",
      "Feature Columns:\n",
      " Index(['Temperatur_Kategorie_Mittel', 'Temperatur_Kategorie_Niedrig',\n",
      "       'Bewoelkung_1.0', 'Bewoelkung_2.0', 'Bewoelkung_3.0', 'Bewoelkung_4.0',\n",
      "       'Bewoelkung_5.0', 'Bewoelkung_6.0', 'Bewoelkung_7.0', 'Bewoelkung_8.0'],\n",
      "      dtype='object')\n",
      "Target Columns:\n",
      " Index(['Warengruppe_1.0', 'Warengruppe_2.0', 'Warengruppe_3.0',\n",
      "       'Warengruppe_4.0', 'Warengruppe_5.0', 'Warengruppe_6.0'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Features aus dem Datensatz herausziehen (hier nur beispielhaft, muss erweitert werden), die ins neuronale Netz eingehen sollen (es scheinen alle am Ende Kategorial zu sein bei uns)\n",
    "categorical_features = ['Temperatur_Kategorie', 'Bewoelkung']\n",
    "\n",
    "# Überprüfen der einzigartigen Werte in den kategorialen Features und umwandlung in Type category falls es nicht schon vorher so war\n",
    "print(\"Unique Values:\\n\",dataf[categorical_features].apply(lambda x: x.unique()))\n",
    "for col in categorical_features:\n",
    "    dataf[col] = dataf[col].astype('category')\n",
    "\n",
    "# Die gewählten Features hier in dummies codieren\n",
    "features = pd.get_dummies(dataf[categorical_features], drop_first=True, dtype=int)\n",
    "\n",
    "# Dependend Varibale (Bei uns der Umsatz pro Warengruppe) mit den Features zusammenfügen\n",
    "prepared_data = pd.concat([pivot_dataf, features], axis=1)\n",
    "\n",
    "# missing values werden entfernt\n",
    "prepared_data = prepared_data.dropna()\n",
    "\n",
    "#DF nach Datum sortieren\n",
    "prepared_data = prepared_data.sort_values(by='Datum')\n",
    "\n",
    "# Display the shape of the prepared data set\n",
    "print(\"Prepared data shape:\\n\", prepared_data.shape)\n",
    "# Display the first few rows of the prepared data set\n",
    "print(\"Prepared data first rows:\\n\", prepared_data.head())\n",
    "\n",
    "# Define the feature columns and target columns\n",
    "feature_columns = ['Temperatur_Kategorie', 'Bewoelkung']\n",
    "feature_columns = prepared_data.columns[prepared_data.columns.str.startswith('Temperatur_Kategorie') | prepared_data.columns.str.startswith('Bewoelkung')]\n",
    "target_columns = prepared_data.columns[prepared_data.columns.str.startswith('Warengruppe_')]\n",
    "\n",
    "print(\"Feature Columns:\\n\", feature_columns)\n",
    "print(\"Target Columns:\\n\", target_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split des Datensatzes in Trainings und Validierungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Datum  Warengruppe_1.0  Warengruppe_2.0  Warengruppe_3.0  \\\n",
      "0 2013-07-01       148.828353       535.856285       201.198426   \n",
      "1 2013-07-02       159.793757       546.780787       265.261254   \n",
      "2 2013-07-03       111.885594       427.343259       210.260241   \n",
      "3 2013-07-04       168.864941       454.859641       190.686641   \n",
      "4 2013-07-05       171.280754       492.818804       181.644870   \n",
      "\n",
      "   Warengruppe_4.0  Warengruppe_5.0  Warengruppe_6.0  \\\n",
      "0        65.890169       317.475875              0.0   \n",
      "1        74.543917       383.628682              0.0   \n",
      "2        69.262728       305.523072              0.0   \n",
      "3        61.490175       308.408168              0.0   \n",
      "4        86.759861       355.518770              0.0   \n",
      "\n",
      "   Temperatur_Kategorie_Mittel  Temperatur_Kategorie_Niedrig  Bewoelkung_1.0  \\\n",
      "0                            0                             1               0   \n",
      "1                            0                             1               0   \n",
      "2                            0                             1               0   \n",
      "3                            0                             1               0   \n",
      "4                            0                             1               0   \n",
      "\n",
      "   Bewoelkung_2.0  Bewoelkung_3.0  Bewoelkung_4.0  Bewoelkung_5.0  \\\n",
      "0               0               0               0               0   \n",
      "1               0               0               0               0   \n",
      "2               0               0               0               0   \n",
      "3               0               0               1               0   \n",
      "4               0               0               0               0   \n",
      "\n",
      "   Bewoelkung_6.0  Bewoelkung_7.0  Bewoelkung_8.0  \n",
      "0               0               0               1  \n",
      "1               0               1               0  \n",
      "2               0               0               1  \n",
      "3               0               0               0  \n",
      "4               1               0               0  \n",
      "          Datum  Warengruppe_1.0  Warengruppe_2.0  Warengruppe_3.0  \\\n",
      "1462 2017-08-01       166.135486       557.470898       294.872701   \n",
      "1463 2017-08-02       155.022099       584.846712       304.116110   \n",
      "1464 2017-08-03       159.575340       483.068232       290.500489   \n",
      "1465 2017-08-04       161.585239       542.553671       296.329976   \n",
      "1466 2017-08-05       198.088094       641.275312       323.640427   \n",
      "\n",
      "      Warengruppe_4.0  Warengruppe_5.0  Warengruppe_6.0  \\\n",
      "1462        88.208006       325.864228              0.0   \n",
      "1463       131.702254       346.233880              0.0   \n",
      "1464        55.423123       284.067056              0.0   \n",
      "1465       109.238985       306.240427              0.0   \n",
      "1466        85.129318       305.981167              0.0   \n",
      "\n",
      "      Temperatur_Kategorie_Mittel  Temperatur_Kategorie_Niedrig  \\\n",
      "1462                            0                             1   \n",
      "1463                            0                             1   \n",
      "1464                            0                             1   \n",
      "1465                            0                             1   \n",
      "1466                            0                             1   \n",
      "\n",
      "      Bewoelkung_1.0  Bewoelkung_2.0  Bewoelkung_3.0  Bewoelkung_4.0  \\\n",
      "1462               0               0               0               0   \n",
      "1463               0               0               0               0   \n",
      "1464               0               0               0               0   \n",
      "1465               0               0               0               0   \n",
      "1466               0               0               0               0   \n",
      "\n",
      "      Bewoelkung_5.0  Bewoelkung_6.0  Bewoelkung_7.0  Bewoelkung_8.0  \n",
      "1462               0               0               0               1  \n",
      "1463               0               0               1               0  \n",
      "1464               0               0               1               0  \n",
      "1465               0               0               1               0  \n",
      "1466               0               0               1               0  \n",
      "Training dataset dimensions: (1462, 17)\n",
      "Validation dataset dimensions: (357, 17)\n",
      "Training features dimensions: (1462, 10)\n",
      "Validation features dimensions: (357, 10)\n",
      "\n",
      "Training labels dimensions: (1462, 6)\n",
      "Validation labels dimensions: (357, 6)\n"
     ]
    }
   ],
   "source": [
    "# Hier muss vermutlich nochmnal eine Randomisierung rein (durchmischen damit nicht zufällig in einem Batch zusammenhängende Daten landen)\n",
    "# vermutlich nach dem auftrennen der Datensätze in die zwei Gruppen (da geht Datum eh verloren)\n",
    "\n",
    "\n",
    "# Definieren der Datumsgrenzen\n",
    "train_start_date = '2013-07-01'\n",
    "train_end_date = '2017-07-31'\n",
    "validation_end_date = '2018-07-31'\n",
    "\n",
    "# Split the data into training (3 years) and validation (1 year)\n",
    "train_data = prepared_data[(prepared_data['Datum']>= train_start_date) & (prepared_data['Datum'] <= train_end_date)]\n",
    "val_data = prepared_data[(prepared_data['Datum']> train_end_date) & (prepared_data['Datum'] <= validation_end_date)]\n",
    "\n",
    "print(train_data.head())\n",
    "print(val_data.head())\n",
    "\n",
    "# Überprüfen der Dimensionen der Datensätze\n",
    "print(\"Training dataset dimensions:\", train_data.shape)\n",
    "print(\"Validation dataset dimensions:\", val_data.shape)\n",
    "\n",
    "# Separating features and labels/targets\n",
    "training_features = train_data[feature_columns].values\n",
    "validation_features = val_data[feature_columns].values\n",
    "\n",
    "training_labels = train_data[target_columns].values\n",
    "validation_labels = val_data[target_columns].values\n",
    "\n",
    "# Print dimensions of the features and labels\n",
    "print(\"Training features dimensions:\", training_features.shape)\n",
    "print(\"Validation features dimensions:\", validation_features.shape)\n",
    "print()\n",
    "print(\"Training labels dimensions:\", training_labels.shape)\n",
    "print(\"Validation labels dimensions:\", validation_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(subdirectory, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Export of the prepared data to subdirectory as pickle files\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtraining_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pickle\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/training_features.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m validation_features\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/validation_features.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m training_labels\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/training_labels.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_pickle'"
     ]
    }
   ],
   "source": [
    "# Create subdirectory for the pickle files -> dadurch behalten die Daten ihre Types\n",
    "subdirectory = \"pickle_data\"\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Export of the prepared data to subdirectory as pickle files\n",
    "\n",
    "training_features.to_pickle(f\"{subdirectory}/training_features.pkl\")\n",
    "validation_features.to_pickle(f\"{subdirectory}/validation_features.pkl\")\n",
    "training_labels.to_pickle(f\"{subdirectory}/training_labels.pkl\")\n",
    "validation_labels.to_pickle(f\"{subdirectory}/validation_labels.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
